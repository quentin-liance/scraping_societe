{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce code, des données sont extraites de pages web pour chaque entreprise listée dans `liste_url`, en utilisant les bibliothèques **requests**, **BeautifulSoup (bs)** et **pandas**. Le code a été mis à jour pour inclure une sélection aléatoire de l'en-tête User-Agent, ce qui peut aider à prévenir le blocage par certains sites web qui limitent les accès automatisés.\n",
    "\n",
    "1. **Initialisation d'une liste pour stocker les données**  `donnees_entreprises = []`\n",
    "\t\n",
    "2. **Boucle sur chaque URL** dans `liste_url` :\n",
    "\t- **Sélection aléatoire d'un User-Agent** : Un navigateur est choisi aléatoirement parmi une liste prédéfinie (`csts.BROWSERS`). Cela permet de varier l'en-tête User-Agent envoyé avec chaque requête, ce qui peut éviter de se faire bloquer par les mesures anti-scraping des sites web.\n",
    "\t- **Envoi d'une requête HTTP GET** : Une requête est envoyée à l'URL avec l'en-tête User-Agent spécifié.\n",
    "\t- **Vérification du statut de la réponse** : Le code vérifie que la réponse a un statut HTTP 200, qui indique une réponse réussie.\n",
    "\t- **Extraction du contenu HTML** : Le contenu HTML de la réponse est stocké.\n",
    "\n",
    "3. **Analyse et extraction des données avec BeautifulSoup**\n",
    "\t-\t**Analyse du contenu HTML :** Le contenu HTML est analysé à l'aide de BeautifulSoup.\n",
    "\t- **Extraction des informations spécifiques** : Le code extrait le nom de l'entreprise, l'adresse, et les numéros SIREN, SIRET et TVA à partir des éléments HTML spécifiés.\n",
    "\n",
    "4. **Création d'un dataframe pour chaque entreprise** : Les informations extraites sont stockées dans un dataframe pandas.\n",
    "\t\n",
    "5.  **Ajout du dataframe à la liste** : Le dataframe créé pour chaque entreprise est ajouté à la liste `donnees_entreprises`.\n",
    "\n",
    "7. **Concaténation des dataframes en un seul** : Après avoir traité toutes les URLs, tous les dataframes sont combinés en un seul dataframe. Cela est fait en utilisant `pd.concat`, qui concatène les dataframes listés, avec `axis=0` pour concaténer verticalement et `ignore_index=True` pour réinitialiser les indices dans le dataframe final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des packages & modules nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import src.constants as csts\n",
    "import src.paths as paths\n",
    "from src.utils import get_current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des headers\n",
    "with open(\"headers.yml\") as f_headers:\n",
    "    browser_headers = yaml.safe_load(f_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test avec 2 URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'avenir, ces listes seront remplacées par un fichier excel contenant 2 colonnes : une colonne \"Nom\" et une colonne \"SIREN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Liste de noms avant préparation : ['France Croco', 'Wahaus']\n",
      ">> Liste de noms après préparation : ['france-croco', 'wahaus']\n",
      ">> Numéros SIREN : ['501737761', '537443228']\n",
      "['https://www.societe.com/societe/france-croco-501737761.html', 'https://www.societe.com/societe/wahaus-537443228.html']\n"
     ]
    }
   ],
   "source": [
    "# Nom \"brutes\"\n",
    "liste_nom = [\"France Croco\", \"Wahaus\"]\n",
    "print(\">> Liste de noms avant préparation :\", liste_nom)\n",
    "\n",
    "# Noms formatés pour les injecter dans l'URL\n",
    "liste_nom = [nom.strip().lower().replace(\" \", \"-\") for nom in liste_nom]\n",
    "print(\">> Liste de noms après préparation :\", liste_nom)\n",
    "\n",
    "# Numéros SIREN\n",
    "liste_numero_siren = [\"501737761\", \"537443228\"]\n",
    "print(\">> Numéros SIREN :\", liste_numero_siren)\n",
    "\n",
    "# Création de la liste des urls\n",
    "liste_url = [csts.BASE_URL + nom + \"-\" + siren + csts.HTML for nom, siren in list(zip(liste_nom, liste_numero_siren))]\n",
    "print(liste_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lancement du scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'extraction des données des entreprises.\n",
      " >> Traitement de l'URL : https://www.societe.com/societe/france-croco-501737761.html\n",
      "    ** Navigateur choisi : Chrome\n",
      "    ** Pause de 10 secondes...\n",
      "    ** Nom de l'entreprise extrait : FRANCE CROCO\n",
      "    ** Adresse extraite : ZA LA MARE AUX RAINES NORD 50190 PERIERS\n",
      "    ** Numéro SIREN extrait : 501737761\n",
      "    ** Numéro SIRET extrait : 50173776100020\n",
      "    ** Numéro TVA extrait : FR81501737761\n",
      " >> Traitement de l'URL : https://www.societe.com/societe/wahaus-537443228.html\n",
      "    ** Navigateur choisi : Firefox\n",
      "    ** Pause de 10 secondes...\n",
      "    ** Nom de l'entreprise extrait : WAHAUS\n",
      "    ** Adresse extraite : 31 AV D EYLAU 75116 PARIS\n",
      "    ** Numéro SIREN extrait : 537443228\n",
      "    ** Numéro SIRET extrait : 53744322800011\n",
      "    ** Numéro TVA extrait : FR61537443228\n",
      "Toutes les données ont été extraites et concaténées dans un dataframe unique.\n"
     ]
    }
   ],
   "source": [
    "# Création d'une liste vide pour stocker les données de chaque entreprise\n",
    "donnees_entreprises = []\n",
    "\n",
    "print(\"Début de l'extraction des données des entreprises.\")\n",
    "\n",
    "# Boucle sur chaque URL dans la liste `liste_url`\n",
    "for url in liste_url:\n",
    "    try:\n",
    "        print(f\" >> Traitement de l'URL : {url}\")\n",
    "\n",
    "        # Choix aléatoire d'un nom de navigateur dans la liste des navigateurs disponibles\n",
    "        browser_name = random.choice(csts.BROWSERS)\n",
    "        print(f\"    ** Navigateur choisi : {browser_name}\")\n",
    "\n",
    "        # Récupération des headers correspondants au navigateur choisi\n",
    "        headers = browser_headers[browser_name]\n",
    "\n",
    "        # Envoi d'une requête HTTP GET à l'URL avec un User-Agent spécifié\n",
    "        response = requests.get(url=url, headers=headers)\n",
    "\n",
    "        # Met le programme en pause pendant 10 secondes.\n",
    "        print(\"    ** Pause de 10 secondes...\")\n",
    "        time.sleep(csts.TIME_SLEEP)\n",
    "\n",
    "        # Obtention du contenu HTML de la réponse\n",
    "        html = response.content\n",
    "\n",
    "        # Analyse du contenu HTML avec BeautifulSoup\n",
    "        contenu_page = bs(html, \"html.parser\")\n",
    "\n",
    "        # Extraction de la première occurrence d'une table avec les classes spécifiées\n",
    "        contenu_page = contenu_page.find_all(\n",
    "            name=\"table\",\n",
    "            class_=\"Table identity mt-16\",\n",
    "        )[0]\n",
    "\n",
    "        # Extraction du nom de l'entreprise\n",
    "        nom = contenu_page.find(name=\"td\", class_=\"break-word\").text\n",
    "        print(f\"    ** Nom de l'entreprise extrait : {nom}\")\n",
    "\n",
    "        # Extraction de l'adresse\n",
    "        adresse = contenu_page.find(name=\"a\", class_=\"Lien secondaire\").text.strip()\n",
    "        print(f\"    ** Adresse extraite : {adresse}\")\n",
    "\n",
    "        # Extraction du numéro SIREN\n",
    "        numero_siren = contenu_page.find(name=\"div\", id=\"siren_number\").text\n",
    "        print(f\"    ** Numéro SIREN extrait : {numero_siren}\")\n",
    "\n",
    "        # Extraction du numéro SIRET\n",
    "        numero_siret = contenu_page.find(name=\"div\", id=\"siret_number\").text\n",
    "        print(f\"    ** Numéro SIRET extrait : {numero_siret}\")\n",
    "\n",
    "        # Extraction du numéro TVA\n",
    "        numero_tva = contenu_page.find(name=\"div\", id=\"tva_number\").text\n",
    "        print(f\"    ** Numéro TVA extrait : {numero_tva}\")\n",
    "\n",
    "        # Création d'un dataframe avec les données extraites\n",
    "        donnees_entreprise = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"Nom Commerciaux\": nom,\n",
    "                    \"Adresse\": adresse,\n",
    "                    \"Numéro SIREN\": numero_siren,\n",
    "                    \"Numéro SIRET\": numero_siret,\n",
    "                    \"Numéro TVA\": numero_tva,\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ajout du dataframe à la liste des dataframes des entreprises\n",
    "        donnees_entreprises.append(donnees_entreprise)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "# Concaténation de tous les dataframes en un seul\n",
    "donnees_entreprises = pd.concat(objs=donnees_entreprises, axis=0, ignore_index=True)\n",
    "print(\"Toutes les données ont été extraites et concaténées dans un dataframe unique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export au format .XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_current_date() + paths.EXCEL_FILE_NAME\n",
    "file_path = paths.DATA_DIR_PATH / file_name\n",
    "donnees_entreprises.to_excel(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping-societe-GP-apnY6-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
